{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycpcy9d1uw6c"
      },
      "outputs": [],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install transformers datasets\n",
        "\n",
        "import torch\n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, AdamW)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Load the dataset (replace with your dataset)\n",
        "fake_df = pd.read_csv('/path/to/your/fake_news.csv')\n",
        "true_df = pd.read_csv('/path/to/your/true_news.csv')\n",
        "\n",
        "# Add labels (0 = fake, 1 = true)\n",
        "fake_df['label'] = 0\n",
        "true_df['label'] = 1\n",
        "\n",
        "# Combine the datasets\n",
        "df = pd.concat([fake_df, true_df], ignore_index=True)\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model to use: 'bert-base-uncased', 'roberta-base', or 'gpt2'\n",
        "model_name = 'bert-base-uncased'  # Change this to 'roberta-base' or 'gpt2' for other models\n",
        "\n",
        "# Load pre-trained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# If using GPT-2, set pad_token as eos_token since GPT-2 does not have a padding token\n",
        "if 'gpt2' in model_name:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define a custom dataset class\n",
        "class FakeNewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):  # Adjust max_len if needed\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts.iloc[index]\n",
        "        label = self.labels.iloc[index]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create Dataset and DataLoader for training and testing\n",
        "train_dataset = FakeNewsDataset(X_train, y_train, tokenizer, max_len=128)\n",
        "test_dataset = FakeNewsDataset(X_test, y_test, tokenizer, max_len=128)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Adjust batch size for memory\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Load the pre-trained model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Set the pad_token_id in both the tokenizer and the model configuration if using GPT-2\n",
        "if 'gpt2' in model_name:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Initialize the GradScaler for mixed precision training\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Helper function to calculate evaluation metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    roc_auc = roc_auc_score(y_true, y_pred)\n",
        "    return accuracy, precision, recall, f1, roc_auc\n",
        "\n",
        "# Training function with mixed precision and gradient accumulation\n",
        "def train(model, data_loader, optimizer, device, accumulation_steps=2):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    for i, batch in enumerate(tqdm(data_loader, desc=\"Training\")):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with autocast():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        # Backward pass with gradient accumulation\n",
        "        scaler.scale(loss / accumulation_steps).backward()\n",
        "\n",
        "        # Step optimizer every 'accumulation_steps' batches\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "# Collect results\n",
        "results = []\n",
        "\n",
        "# Training and evaluation loop\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    # Time the training process\n",
        "    start_train_time = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, device)\n",
        "    train_time = time.time() - start_train_time\n",
        "    print(f\"Training loss: {train_loss}\")\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    start_inference_time = time.time()\n",
        "    test_predictions, test_true_labels = evaluate(model, test_loader, device)\n",
        "    inference_time = time.time() - start_inference_time\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_acc, test_prec, test_rec, test_f1, test_roc_auc = calculate_metrics(test_true_labels, test_predictions)\n",
        "\n",
        "    # Log parameter count\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    # Collect results\n",
        "    results.append({\n",
        "        'Epoch': epoch + 1,\n",
        "        'Num Parameters': num_params,\n",
        "        'Train Time (s)': train_time,\n",
        "        'Inference Time (s)': inference_time,\n",
        "        'Test Accuracy': test_acc,\n",
        "        'Test Precision': test_prec,\n",
        "        'Test Recall': test_rec,\n",
        "        'Test F1': test_f1,\n",
        "        'Test ROC-AUC': test_roc_auc\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame and save\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('/path/to/save/your_results.csv', index=False)\n",
        "\n",
        "# Print the results\n",
        "print(results_df)\n"
      ]
    }
  ]
}